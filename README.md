# Emotion-and-Speech-Chatbot
Using the MIT App Inventor, I combined MIT APP inventor's Personal Image Classifier and Speech Recognition to output a simple solution for the user based on the speech input and the facial emotion.

# How I made it
The MIT App Inventor introduces a feature called the Personal Image Classifier, where users can use MIT App Inventor's model to learn from a dataset that is presented. Using my own dataset of emotions on my face, I was able to create a model that can predict the emotions of a person based on their facial expression. The confidence level is still really low because of the limited dataset available. After retrieving the results of the emotional analysis from the model, I used the Speech Recognition program to record and transcribe the audio of the user, mainly looking for sentence structures, grammar indications, and questions/concerns of the user. After the transcript is recieved, both results will be ran through an AI using a personal API key. The preinputed prompt will make the answers of the AI concist and supportive. 

# How to use
To get started with the project, simply download the .aia and import it to the website. AFter that, connect through an AI companion and scan the QR Code on the MIT App Inventor on the mobile app.

# Future Developments
I'm hoping to be able to retrieve a much bigger amount of datasets to be able to increase the confidence rate. I'm hoping to be able to aid mentally challenged children learn expression and communication with AI. 

# This simple project was made by me
As a simple way to learn about datasets and the MIT App Inventor after watching the 2025 Hackathon finalists. 
